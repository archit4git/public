{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe Builder Actions Overview\n",
    "\n",
    "### Saving a File Cell\n",
    "If you wish to save the contents of a cell, simply run it.  The `%%writefile` command at the top of the cell will write the contents of the cell to the file named at the top of the cell. You should run the cells manually when applicable. However, **pressing any of the actions at the top will automatically run all file cells relevant to the action**.\n",
    "\n",
    "### Training and Scoring\n",
    "Press the associated buttons at the top in order to run training or scoring. The training output will be shown below the `evaluator.py` cell and scoring output will be shown below the `datasaver.py` cell. You must run training at least once before you can run scoring. You may delete the output cell(s). Running training the first time or after changing `requirements.txt` will be slower since the dependencies for the recipe need to be installed, but subsequent runs will be significantly faster. If you wish to see the hidden output add `debug` to the end of the output cell and re-run it.\n",
    "\n",
    "### Creating the Recipe\n",
    "When you are done editing the recipe and satisfied with the training/scoring output, you can create a recipe from the notebook by pressing `Create Recipe`. You must run scoring at least once before you can create the recipe. After pressing it, you will see a progress bar showing how much time is left for the build to finish. If the recipe creation is successful the progress bar will be replaced by an external link that you can click to navigate to the created recipe.\n",
    "\n",
    "\n",
    "## Caution!\n",
    "* **Do not delete any of the file cells**\n",
    "* **Do not edit the `%%writefile` line at the top of the file cells**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Requirements File** (Optional)\n",
    "Add additional libraries you wish to use in the recipe to the cell below. You can specify the version number if necessary. The file cell below is a **commented out example**.  The file structure is yaml. It adheres to the specification outlined in the [Conda Documentation](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-file-manually). **NOTE:** The name field is not allowed to be overridden in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "requirements.txt",
    "tags": [
     "requirements.txt"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-cyiuzaLij/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-cyiuzaLij/requirements.txt\n",
    "\n",
    "#dependencies:\n",
    "## Conda dependencies should be attempted first\n",
    "#- statsmodels=0.10.2\n",
    "#- pip\n",
    "#- pip:\n",
    "## Pip installs can be listed next but should only be used when Conda is unavailable\n",
    "#   - pmdarima==1.5.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search here for additional libraries https://anaconda.org/. This is the list of main **libraries already in use**:\n",
    "`python=3.6.7` `scikit-learn` `pandas` `numpy` `data_access_sdk_python`\n",
    "**Warning: libraries or specific versions you add may be incompatible with the above libraries**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Configuration Files**\n",
    "List any hyperparameters you wish to use. Specify the dataset(s) and schema(s) that are needed for training/scoring. To find the dataset ids go to the **Data tab** in Adobe Experience Platform or view the **Datasets** folder in the **Notebooks Data tab** on the left. You can also find schema id in the **Notebooks Data tab** under the **Schemas** folder. Each configuration will only be used for its corresponding action. `ACP_DSW_TRAINING_XDM_SCHEMA` and `ACP_DSW_SCORING_RESULTS_XDM_SCHEMA` will only be used after the recipe has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "training.conf",
    "tags": [
     "training.conf"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-cyiuzaLij/training.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-cyiuzaLij/training.conf\n",
    "\n",
    "{\n",
    "   \"trainingDataSetId\": \"605aa0b37d909c194873348a\",\n",
    "   \"ACP_DSW_TRAINING_XDM_SCHEMA\": \"https://ns.adobe.com/trowelab/schemas/c9ed4fec408542746fbc0de1e2f1ebd1cc6fbe378985f3b3\",\n",
    "   \"tenantId\": \"_trowelab\", \n",
    "   \"max_iter\": \"50000\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scoring Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "id": "scoring.conf",
    "tags": [
     "scoring.conf"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-cyiuzaLij/scoring.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-cyiuzaLij/scoring.conf\n",
    "\n",
    "{\n",
    "   \"scoringDataSetId\": \"605aa0eae30ac41949d41637\",\n",
    "   \"scoringResultsDataSetId\": \"605aa30ec251b71948f2dab4\",\n",
    "   \"ACP_DSW_SCORING_RESULTS_XDM_SCHEMA\": \"https://ns.adobe.com/trowelab/schemas/23e6fba6f38b1aea21502439fba5695916c4632e511ecdf4\",\n",
    "   \"tenantId\": \"_trowelab\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following configuration parameters are automatically set for you when you train/score:** \n",
    "`ML_FRAMEWORK_IMS_USER_CLIENT_ID` `ML_FRAMEWORK_IMS_TOKEN` `ML_FRAMEWORK_IMS_ML_TOKEN` `ML_FRAMEWORK_IMS_TENANT_ID`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training Data Loader File**\n",
    "Implement the `load` function to load and prepare the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "trainingdataloader.py",
    "tags": [
     "trainingdataloader.py"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-cyiuzaLij/recipe/trainingdataloader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-cyiuzaLij/recipe/trainingdataloader.py\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from platform_sdk.dataset_reader import DatasetReader\n",
    "from .utils import get_client_context\n",
    "\n",
    "def load(config_properties):\n",
    "    print(\"Training Data Load Start\")\n",
    "\n",
    "    #########################################\n",
    "    # Load Data\n",
    "    #########################################    \n",
    "    client_context = get_client_context(config_properties)\n",
    "    \n",
    "    dataset_reader = DatasetReader(client_context, config_properties['trainingDataSetId'])\n",
    "    \n",
    "    tenant_id = config_properties.get(\"tenant_id\")\n",
    "    \n",
    "    dataframe = dataset_reader.read()\n",
    "\n",
    "    #########################################\n",
    "    # Data Preparation/Feature Engineering\n",
    "    #########################################    \n",
    "    dataframe = dataframe.drop(\"UserID\", axis=1)\n",
    "    dataframe = dataframe.astype('int32')\n",
    "    dataframe = pd.concat([dataframe[\"checked_delivery_detail\"], dataframe[\"saw_checkout\"], dataframe[\"sign_in\"], dataframe[\"ordered\"]], axis=1)\n",
    "\n",
    "    print(\"Training Data Load Finish\")\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Scoring Data Loader File**\n",
    "Implement the `load` function to load and prepare the scoring data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "scoringdataloader.py",
    "tags": [
     "scoringdataloader.py"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-cyiuzaLij/recipe/scoringdataloader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-cyiuzaLij/recipe/scoringdataloader.py\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from .utils import get_client_context\n",
    "from platform_sdk.dataset_reader import DatasetReader\n",
    "\n",
    "def load(config_properties):\n",
    "\n",
    "    print(\"Scoring Data Load Start\")\n",
    "\n",
    "    #########################################\n",
    "    # Load Data\n",
    "    #########################################\n",
    "    client_context = get_client_context(config_properties)\n",
    "\n",
    "    dataset_reader = DatasetReader(client_context, config_properties['scoringDataSetId'])\n",
    "    tenant_id = config_properties.get(\"tenant_id\")\n",
    "\n",
    "    dataframe = dataset_reader.read()\n",
    "\n",
    "    #########################################\n",
    "    # Data Preparation/Feature Engineering\n",
    "    #########################################\n",
    "\n",
    "    dataframe = pd.concat([dataframe[\"UserID\"], dataframe[\"checked_delivery_detail\"].astype('int32'), dataframe[\"saw_checkout\"].astype('int32'), dataframe[\"sign_in\"].astype('int32')], axis=1)\n",
    "    \n",
    "    print(\"Scoring Data Load Finish\")\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline File**\n",
    "Implement the `train` function and return the trained model. Implement the `score` function to return a prediction made on the scoring data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "id": "pipeline.py",
    "tags": [
     "pipeline.py"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-cyiuzaLij/recipe/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-cyiuzaLij/recipe/pipeline.py\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def train(config_properties, data):\n",
    "\n",
    "    print(\"Train Start\")\n",
    "        \n",
    "    #########################################\n",
    "    # Extract fields from configProperties\n",
    "    #########################################\n",
    "    max_iter = int(config_properties['max_iter'])\n",
    "\n",
    "\n",
    "    #########################################\n",
    "    # Fit model\n",
    "    #########################################\n",
    "    X_train = data.drop('ordered', axis=1).values\n",
    "    y_train = data['ordered']\n",
    "\n",
    "    model = svm.SVC(probability=True, max_iter=max_iter).fit(X_train, y_train)\n",
    "\n",
    "    print(\"Train Complete\")\n",
    "    return model\n",
    "\n",
    "def score(config_properties, data, model):\n",
    "\n",
    "    print(\"Score Start\")\n",
    "\n",
    "    X_test = data.drop('UserID', axis=1).values\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    data['prediction'] = y_pred\n",
    "    #data = data[['UserID', 'prediction']].reset_index()\n",
    "\n",
    "    print(\"Score Complete\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluator File**\n",
    "Implement the `split` function to partition the training data and the `evaluate` function to the return the validation metrics you wish to see. Training output will be shown below this file cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "evaluator.py",
    "tags": [
     "evaluator.py"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-cyiuzaLij/recipe/evaluator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-cyiuzaLij/recipe/evaluator.py\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "class Evaluator():\n",
    "    def __init__(self):\n",
    "        print (\"Initiate\")\n",
    "\n",
    "    def split(self, config={}, dataframe=None):\n",
    "        \n",
    "        train = dataframe[0:5000]\n",
    "        val = dataframe[5000:]\n",
    "\n",
    "        return train, val\n",
    "\n",
    "    def evaluate(self, data=[], model={}, config={}):\n",
    "        print (\"Evaluation triggered\")\n",
    "\n",
    "        val = data.drop('ordered', axis=1)\n",
    "        y_pred = model.predict(val)\n",
    "        y_actual = data['ordered'].values\n",
    "             \n",
    "        acc = model.score(val, y_actual)\n",
    "        recall = recall_score(y_actual, y_pred)\n",
    "        precision = precision_score(y_actual, y_pred)\n",
    "\n",
    "        metric = [{\"name\": \"Accuracy\", \"value\": acc, \"valueType\": \"double\"},\n",
    "                  {\"name\": \"Precision\", \"value\": precision, \"valueType\": \"double\"},\n",
    "                  {\"name\": \"Recall\", \"value\": recall, \"valueType\": \"double\"}]\n",
    "        \n",
    "        print(metric)\n",
    "        return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "training-cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING TRAINING...\n",
      "Ignore the exception for python runtime cannot import name 'BlockBlobService'\n",
      "2021-03-24 04:47:47,837 INFO trainingInitiator.py: Trainer initiated\n",
      "INFO:trainingInitiator.py:Trainer initiated\n",
      "2021-03-24 04:47:47,838 INFO trainingInitiator.py: Evaluator initiated\n",
      "INFO:trainingInitiator.py:Evaluator initiated\n",
      "2021-03-24 04:47:47,838 INFO main.py: Training starts, testing:True, conf:/tmp/tmp.jM29T49Hcf/token.conf\n",
      "INFO:main.py:Training starts, testing:True, conf:/tmp/tmp.jM29T49Hcf/token.conf\n",
      "2021-03-24 04:47:47,838 INFO trainingInitiator.py: Training class is not of type Tensorflow\n",
      "INFO:trainingInitiator.py:Training class is not of type Tensorflow\n",
      "2021-03-24 04:47:47,838 INFO trainingInitiator.py: Python Job\n",
      "INFO:trainingInitiator.py:Python Job\n",
      "2021-03-24 04:47:47,838 INFO trainingInitiator.py: Load the dataframe\n",
      "INFO:trainingInitiator.py:Load the dataframe\n",
      "Training Data Load Start\n",
      "INFO:PlatformSDKPython:dataset_reader: seconds taken to get dataset details from catalog and make PQS connection: 2.01\n",
      "INFO:PlatformSDKPython:dataset_id: 605aa0b37d909c194873348a, limit: 50000\n",
      "INFO:PlatformSDKPython:dataset_reader: seconds taken to execute query: 15.08\n",
      "INFO:PlatformSDKPython:dataset_reader: 7000 rows read. 136.82 MB memory used for this process\n",
      "INFO:PlatformSDKPython:dataset_reader: seconds taken to format data of dataframe: 0.04\n",
      "Training Data Load Finish\n",
      "2021-03-24 04:48:04,988 INFO trainingInitiator.py: Evaluator class is defined for python\n",
      "INFO:trainingInitiator.py:Evaluator class is defined for python\n",
      "Initiate\n",
      "2021-03-24 04:48:04,988 INFO trainingInitiator.py: Python Training started\n",
      "INFO:trainingInitiator.py:Python Training started\n",
      "Train Start\n",
      "checked_delivery_detail    int32\n",
      "saw_checkout               int32\n",
      "sign_in                    int32\n",
      "ordered                    int32\n",
      "dtype: object\n",
      "/opt/conda/envs/usermlruntimepython/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "Train Complete\n",
      "2021-03-24 04:48:05,139 INFO Helper.py: Save Model completed : /home/asruser/my-workspace/.recipes/trainedModels/recipe-cyiuzaLij/trainedModel\n",
      "INFO:Helper.py:Save Model completed : /home/asruser/my-workspace/.recipes/trainedModels/recipe-cyiuzaLij/trainedModel\n",
      "2021-03-24 04:48:05,139 INFO trainingInitiator.py: Python Training completed\n",
      "INFO:trainingInitiator.py:Python Training completed\n",
      "2021-03-24 04:48:05,140 INFO trainingInitiator.py: Evaluation will be on the test data\n",
      "INFO:trainingInitiator.py:Evaluation will be on the test data\n",
      "2021-03-24 04:48:05,140 INFO trainingInitiator.py: Evaluate config is set to true\n",
      "INFO:trainingInitiator.py:Evaluate config is set to true\n",
      "2021-03-24 04:48:05,140 INFO evaluateInitiator.py: Starting evaluation\n",
      "INFO:evaluateInitiator.py:Starting evaluation\n",
      "Initiate\n",
      "Evaluation triggered\n",
      "/opt/conda/envs/usermlruntimepython/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "[{'name': 'Accuracy', 'value': 0.9945, 'valueType': 'double'}, {'name': 'Precision', 'value': 0.0, 'valueType': 'double'}, {'name': 'Recall', 'value': 0.0, 'valueType': 'double'}]\n",
      "2021-03-24 04:48:05,155 INFO evaluateInitiator.py: Evaluation completed\n",
      "INFO:evaluateInitiator.py:Evaluation completed\n",
      "TRAINING SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "!bash -e run_action.sh recipe-cyiuzaLij training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Saver File**\n",
    "Implement the `save` function for saving your prediction. Scoring output will be added below this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "datasaver.py",
    "tags": [
     "datasaver.py"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-cyiuzaLij/recipe/datasaver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-cyiuzaLij/recipe/datasaver.py\n",
    "\n",
    "import pandas as pd\n",
    "from .utils import get_client_context\n",
    "from platform_sdk.models import Dataset\n",
    "from platform_sdk.dataset_writer import DatasetWriter\n",
    "\n",
    "def save(config_properties, prediction):\n",
    "  print(\"Datasaver Start\")\n",
    "\n",
    "  client_context = get_client_context(config_properties)\n",
    "  tenant_id = config_properties.get(\"tenantId\")\n",
    "\n",
    "  dataset = Dataset(client_context).get_by_id(config_properties['scoringResultsDataSetId'])\n",
    "  dataset_writer = DatasetWriter(client_context, dataset)\n",
    "  dataset_writer.write(prediction, file_format='json')\n",
    "\n",
    "  print(\"Datasaver Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "id": "scoring-cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING SCORING...\n",
      "Ignore the exception for python runtime cannot import name 'BlockBlobService'\n",
      "2021-03-24 04:48:58,106 INFO main.py: Scoring starts, testing:True, conf:/tmp/tmp.5pyuk9EYbg/token.conf\n",
      "INFO:main.py:Scoring starts, testing:True, conf:/tmp/tmp.5pyuk9EYbg/token.conf\n",
      "2021-03-24 04:48:58,665 INFO scoringInitiator.py: Scorer initiated\n",
      "INFO:scoringInitiator.py:Scorer initiated\n",
      "2021-03-24 04:48:58,666 INFO scoringInitiator.py: Scoring class is not of type Tensorflow\n",
      "INFO:scoringInitiator.py:Scoring class is not of type Tensorflow\n",
      "2021-03-24 04:48:58,666 INFO scoringInitiator.py: Python scoring starts\n",
      "INFO:scoringInitiator.py:Python scoring starts\n",
      "2021-03-24 04:48:58,666 INFO scoringInitiator.py: Python executed scoring\n",
      "INFO:scoringInitiator.py:Python executed scoring\n",
      "Scoring Data Load Start\n",
      "INFO:PlatformSDKPython:dataset_reader: seconds taken to get dataset details from catalog and make PQS connection: 1.58\n",
      "INFO:PlatformSDKPython:dataset_id: 605aa0eae30ac41949d41637, limit: 50000\n",
      "INFO:PlatformSDKPython:dataset_reader: seconds taken to execute query: 10.48\n",
      "INFO:PlatformSDKPython:dataset_reader: 366 rows read. 126.1 MB memory used for this process\n",
      "INFO:PlatformSDKPython:dataset_reader: seconds taken to format data of dataframe: 0.02\n",
      "Scoring Data Load Finish\n",
      "Score Start\n",
      "UserID                     object\n",
      "checked_delivery_detail     int32\n",
      "saw_checkout                int32\n",
      "sign_in                     int32\n",
      "dtype: object\n",
      "Score Complete\n",
      "Datasaver Start\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://login.microsoftonline.com/fa7b1b5a-7b34-4387-94ae-d2c178decee1/oauth2/v2.0/token'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request method: 'POST'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request headers:\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Content-Type': 'application/x-www-form-urlencoded'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'User-Agent': 'azsdk-python-identity/1.4.0 Python/3.6.7 (Linux-5.4.0-1039-azure-x86_64-with-debian-stretch-sid)'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:A body is sent with the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 200\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response headers:\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Cache-Control': 'no-store, no-cache'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Pragma': 'no-cache'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Content-Length': '2215'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Content-Type': 'application/json; charset=utf-8'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Expires': '-1'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Strict-Transport-Security': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'X-Content-Type-Options': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'P3P': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-request-id': 'ba8d2570-701e-4548-ac3a-6ba9bf122500'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-ests-server': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Set-Cookie': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Date': 'Wed, 24 Mar 2021 04:49:11 GMT'\n",
      "INFO:azure.identity._internal.decorators:ClientSecretCredential.get_token succeeded\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://datalakex4iap86u188anno1.dfs.core.windows.net/gen1/foundation%2Fdata%2Fstage%2Fusers%2F10370DFA5DD301D40A495FEB%40AdobeID%2F01F1H91AEJDRBT8SMDBPVPMKD7%2F605aa30ec251b71948f2dab5?resource=REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request method: 'PUT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request headers:\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-properties': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-version': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'If-None-Match': '*'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-date': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-client-request-id': '4707c854-8c5c-11eb-948a-9a4712bcb105'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'User-Agent': 'azsdk-python-storage-dfs/12.0.1 Python/3.6.7 (Linux-5.4.0-1039-azure-x86_64-with-debian-stretch-sid)'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Authorization': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:No body was attached to the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 201\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response headers:\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Last-Modified': 'Wed, 24 Mar 2021 04:49:12 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'ETag': '\"0x8D8EE802B6FFFED\"'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Server': 'Windows-Azure-HDFS/1.0 Microsoft-HTTPAPI/2.0'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-request-id': '553410b9-d01f-0048-8069-20d236000000'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-version': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-client-request-id': '4707c854-8c5c-11eb-948a-9a4712bcb105'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Date': 'Wed, 24 Mar 2021 04:49:12 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Content-Length': '0'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://datalakex4iap86u188anno1.dfs.core.windows.net/gen1/foundation%2Fdata%2Fstage%2Fusers%2F10370DFA5DD301D40A495FEB%40AdobeID%2F01F1H91AEJDRBT8SMDBPVPMKD7%2F605aa30ec251b71948f2dab5%2F1616561352133.json?resource=REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request method: 'PUT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request headers:\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-properties': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-version': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-date': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-client-request-id': '473d3f98-8c5c-11eb-948a-9a4712bcb105'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'User-Agent': 'azsdk-python-storage-dfs/12.0.1 Python/3.6.7 (Linux-5.4.0-1039-azure-x86_64-with-debian-stretch-sid)'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Authorization': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:No body was attached to the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 201\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response headers:\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Last-Modified': 'Wed, 24 Mar 2021 04:49:12 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'ETag': '\"0x8D8EE802B7BE902\"'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Server': 'Windows-Azure-HDFS/1.0 Microsoft-HTTPAPI/2.0'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-request-id': '553410c2-d01f-0048-0969-20d236000000'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-version': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-client-request-id': '473d3f98-8c5c-11eb-948a-9a4712bcb105'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Date': 'Wed, 24 Mar 2021 04:49:12 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Content-Length': '0'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://datalakex4iap86u188anno1.dfs.core.windows.net/gen1/foundation%2Fdata%2Fstage%2Fusers%2F10370DFA5DD301D40A495FEB%40AdobeID%2F01F1H91AEJDRBT8SMDBPVPMKD7%2F605aa30ec251b71948f2dab5%2F1616561352133.json?position=REDACTED&action=REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request method: 'PATCH'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request headers:\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Content-Type': 'application/json; charset=utf-8'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Content-Length': '47543'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-version': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-date': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-client-request-id': '474942de-8c5c-11eb-948a-9a4712bcb105'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'User-Agent': 'azsdk-python-storage-dfs/12.0.1 Python/3.6.7 (Linux-5.4.0-1039-azure-x86_64-with-debian-stretch-sid)'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Authorization': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:A body is sent with the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 202\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response headers:\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Server': 'Windows-Azure-HDFS/1.0 Microsoft-HTTPAPI/2.0'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-request-server-encrypted': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-request-id': '553410c9-d01f-0048-1069-20d236000000'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-version': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-client-request-id': '474942de-8c5c-11eb-948a-9a4712bcb105'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Date': 'Wed, 24 Mar 2021 04:49:12 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Content-Length': '0'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://datalakex4iap86u188anno1.dfs.core.windows.net/gen1/foundation%2Fdata%2Fstage%2Fusers%2F10370DFA5DD301D40A495FEB%40AdobeID%2F01F1H91AEJDRBT8SMDBPVPMKD7%2F605aa30ec251b71948f2dab5%2F1616561352133.json?position=REDACTED&action=REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request method: 'PATCH'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request headers:\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-version': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'If-Match': '\"0x8D8EE802B7BE902\"'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-date': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-client-request-id': '474ca636-8c5c-11eb-948a-9a4712bcb105'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'User-Agent': 'azsdk-python-storage-dfs/12.0.1 Python/3.6.7 (Linux-5.4.0-1039-azure-x86_64-with-debian-stretch-sid)'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Authorization': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:No body was attached to the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 200\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response headers:\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Last-Modified': 'Wed, 24 Mar 2021 04:49:12 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'ETag': '\"0x8D8EE802B84E598\"'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Server': 'Windows-Azure-HDFS/1.0 Microsoft-HTTPAPI/2.0'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-request-server-encrypted': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-request-id': '553410cc-d01f-0048-1369-20d236000000'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-version': 'REDACTED'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'x-ms-client-request-id': '474ca636-8c5c-11eb-948a-9a4712bcb105'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Date': 'Wed, 24 Mar 2021 04:49:12 GMT'\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:    'Content-Length': '0'\n",
      "INFO:PlatformSDKPython:dataset_writer: 366 rows written. 129.01 MB memory used for this process\n",
      "Datasaver Finish\n",
      "2021-03-24 04:49:13,100 INFO scoringInitiator.py: Python scoring completed\n",
      "INFO:scoringInitiator.py:Python scoring completed\n",
      "SCORING SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "!bash -e run_action.sh recipe-cyiuzaLij scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "elementId": "88ID8Vb8B",
  "isScoringRun": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notebook_type": "builder",
  "recipe_name": "recipe-cyiuzaLij"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
